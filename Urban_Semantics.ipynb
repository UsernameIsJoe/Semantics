{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa6rfpClIPfp"
      },
      "source": [
        "# ***Map Foundation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud7oPI-Fhh8n"
      },
      "source": [
        "***Install Package***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aLhNuPpU4LMF"
      },
      "outputs": [],
      "source": [
        "!pip install leafmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UKQAt_lWhlLl"
      },
      "outputs": [],
      "source": [
        "!pip install segment-geospatial groundingdino-py leafmap localtileserver\n",
        "!pip install contextily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqaOUjwThxJ4"
      },
      "source": [
        "***Create an Interactive Map***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4HOePq-HqR0"
      },
      "outputs": [],
      "source": [
        "import leafmap\n",
        "from samgeo import tms_to_geotiff\n",
        "from samgeo.text_sam import LangSAM\n",
        "import leafmap.leafmap as leafmap\n",
        "\n",
        "# create map\n",
        "m = leafmap.Map(center=[ 42.374443, -71.116943], zoom=18, height = \"800px\")\n",
        "m.add_basemap(\"SATELLITE\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZRGDug7I6dY"
      },
      "source": [
        "***Get bounding coords***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blec97vKI-kv"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Zoom and move the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map\n",
        "\n",
        "bbox = m.user_roi_bounds()\n",
        "if bbox is None:\n",
        "    bbox = [-122.4611, 37.7636, -122.4488, 37.7713]\n",
        "\n",
        "bbbox = bbox\n",
        "print(bbox)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxe4hXNoI_ai"
      },
      "source": [
        "***Download and show imagery***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDNGmLjUJEto"
      },
      "outputs": [],
      "source": [
        "# Download satellite image\n",
        "image = \"Image.tif\"\n",
        "tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n",
        "\n",
        "# Display the downloaded satellite image on the map\n",
        "m.layers[-1].visible = False\n",
        "m.add_raster(image, layer_name=\"Image\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i2h2smAIYrq"
      },
      "source": [
        "# ***StreetView Collection***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdZC44Uoc2mr"
      },
      "source": [
        "***Download Street Views***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkcbBfjWlmC3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX82EfftaLAB"
      },
      "outputs": [],
      "source": [
        "def generate_random_points(lat_min, lat_max, lon_min, lon_max, num_points):\n",
        "    \"\"\"Generate random latitude and longitude points within a bounding box.\"\"\"\n",
        "    latitudes = np.random.uniform(low=lat_min, high=lat_max, size=num_points)\n",
        "    longitudes = np.random.uniform(low=lon_min, high=lon_max, size=num_points)\n",
        "    return latitudes, longitudes\n",
        "\n",
        "def is_street_view_available(lat, lon, api_key):\n",
        "    \"\"\"Check if Street View imagery is available at a given location using the Metadata API.\"\"\"\n",
        "    url = \"https://maps.googleapis.com/maps/api/streetview/metadata\"\n",
        "    params = {'location': f'{lat},{lon}', 'key': api_key}\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json().get('status') == 'OK'\n",
        "\n",
        "def download_street_view_images(latitudes, longitudes, api_key, num_images, save_path='images'):\n",
        "    \"\"\"Download Google Street View images for given coordinates up to a specified number.\"\"\"\n",
        "    base_url = \"https://maps.googleapis.com/maps/api/streetview\"\n",
        "\n",
        "     # Clear out the existing images in the save path at the start of each run\n",
        "    if os.path.exists(save_path):\n",
        "        for file in os.listdir(save_path):\n",
        "            os.remove(os.path.join(save_path, file))\n",
        "    else:\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    downloaded_count = 0\n",
        "    for lat, lon in zip(latitudes, longitudes):\n",
        "        if downloaded_count >= num_images:\n",
        "            break  # Ensure no more downloads once the target is reached\n",
        "        if is_street_view_available(lat, lon, api_key):\n",
        "            params = {\n",
        "                'size': '640x640',\n",
        "                'location': f'{lat},{lon}',\n",
        "                'key': api_key\n",
        "            }\n",
        "            response = requests.get(base_url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                file_path = os.path.join(save_path, f\"{lat}_{lon}.jpg\")\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                downloaded_count += 1  # Increment only after a successful download\n",
        "                print(f\"Downloaded {downloaded_count} of {num_images}: {file_path}\")\n",
        "            else:\n",
        "                print(f\"Failed to download image at {lat}, {lon}, Status Code: {response.status_code}\")\n",
        "        else:\n",
        "            print(f\"No Street View available at {lat}, {lon}\")\n",
        "\n",
        "\n",
        "\n",
        "def display_images(folder_path='images'):\n",
        "    \"\"\"Display images from a specified directory.\"\"\"\n",
        "    image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.jpg')]\n",
        "    cols = 5\n",
        "    rows = len(image_files) // cols + (len(image_files) % cols > 0)\n",
        "    plt.figure(figsize=(15, 3 * rows))\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        img = Image.open(image_file)\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main(api_key, lat_min, lat_max, lon_min, lon_max, num_images):\n",
        "    latitudes, longitudes = generate_random_points(lat_min, lat_max, lon_min, lon_max, num_images * 5)  # Increase number to ensure sufficient availability\n",
        "    download_street_view_images(latitudes, longitudes, api_key, num_images=num_images)\n",
        "    display_images()\n",
        "\n",
        "# User input for the number of images\n",
        "num_images = int(input(\"type in how many images you need\"))\n",
        "\n",
        "# Define bounding box [lat_min, lat_max, lon_min, lon_max]\n",
        "bbox = [bbbox[1],bbbox[3],bbbox[0],bbbox[2]]\n",
        "\n",
        "# API Key - replace with your actual API key\n",
        "api_key = 'AIzaSyBfcDwI-86zy3jZ00uwnsuxAwUjD8J0kCw'\n",
        "\n",
        "# Execute the main function with the bounding box and number of images\n",
        "main(api_key, *bbox, num_images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGefNqkZSGbN"
      },
      "source": [
        "***Download image folder***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zbj-KskjF37O"
      },
      "outputs": [],
      "source": [
        "!zip -r output.zip images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAwTHuQeGln_"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('output.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ZO1DIuIiYt"
      },
      "source": [
        "# ***SNS Collection***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt-b0SLAl0gC"
      },
      "source": [
        "***Download SNS data by location***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPD3otqH3-d0"
      },
      "source": [
        "***Get search area from bbox***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxnIm4yCqmuD"
      },
      "outputs": [],
      "source": [
        "def calculate_center_and_radius(north, south, east, west):\n",
        "    # Calculate the center latitude and longitude\n",
        "    center_lat = (north + south) / 2\n",
        "    center_lon = (east + west) / 2\n",
        "\n",
        "    # Approximate the radius (assuming Earth's curvature is negligible in small areas)\n",
        "    initial_radius = max(abs(north - south), abs(east - west)) / 2 * 111000  # converting degrees to meters approximately\n",
        "\n",
        "    return f\"{center_lat},{center_lon}\", str(int(initial_radius))\n",
        "    print(f\"{center_lat},{center_lon}, {initial_radius}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP-9dfRU4DoS"
      },
      "source": [
        "***Check if comments are valid***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxQLv-SNosyM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_place_details(api_key, place_id, max_comments):\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "    params = {\n",
        "        \"key\": api_key,\n",
        "        \"place_id\": place_id,\n",
        "        \"fields\": \"reviews\"  # Specify only to fetch reviews\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    details = response.json().get('result', {})\n",
        "\n",
        "    reviews = details.get('reviews', [])\n",
        "    if reviews:\n",
        "        # Limit the number of reviews and format the output\n",
        "        limited_reviews = reviews[:max_comments]\n",
        "        return [review['text'] for review in limited_reviews]\n",
        "    else:\n",
        "        return \"No comments found\"\n",
        "\n",
        "# Adjust the main function or the loop where you call get_place_details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44gNGwe94LId"
      },
      "source": [
        "***Collect comments and adjust the size of commentset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smMr94v1wwWc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def search_places(api_key, location, initial_radius, max_results, max_radius=10000):\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "    radius = initial_radius\n",
        "    places = []\n",
        "    unique_place_ids = set()\n",
        "\n",
        "    while len(places) < max_results and radius <= max_radius:\n",
        "        params = {\n",
        "            \"key\": api_key,\n",
        "            \"location\": location,\n",
        "            \"radius\": radius\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch places: {response.status_code} {response.text}\")\n",
        "            break\n",
        "\n",
        "        results = response.json()\n",
        "        new_places = results.get('results', [])\n",
        "        print(f\"Search radius: {radius} meters, found {len(new_places)} new places.\")\n",
        "\n",
        "        # Add unique places\n",
        "        for place in new_places:\n",
        "            if place['id'] not in unique_place_ids:\n",
        "                unique_place_ids.add(place['id'])\n",
        "                places.append(place)\n",
        "\n",
        "        # Increase radius for next iteration\n",
        "        radius += 1000\n",
        "\n",
        "    print(f\"Total unique places found: {len(places)}\")\n",
        "    return places[:max_results]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7g9VDi98Val"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def search_places(api_key, location, max_results):\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "    params = {\n",
        "        \"key\": api_key,\n",
        "        \"location\": location,  # latitude and longitude as a string\n",
        "        \"rankby\": \"distance\"  # Sort places strictly by distance\n",
        "    }\n",
        "\n",
        "    places = []\n",
        "    while len(places) < max_results:\n",
        "        # Make the API request\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch places: {response.status_code} {response.text}\")\n",
        "            break\n",
        "\n",
        "        results = response.json()\n",
        "        new_places = results.get('results', [])\n",
        "        places.extend(new_places)\n",
        "\n",
        "        # Check for a next page token\n",
        "        next_page_token = results.get('next_page_token')\n",
        "        if not next_page_token or len(places) >= max_results:\n",
        "            break\n",
        "\n",
        "        # Delay required by Google before next page token can be used\n",
        "        import time\n",
        "        time.sleep(2)\n",
        "        params['pagetoken'] = next_page_token\n",
        "\n",
        "    return places[:max_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DreY_g9dvsvj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def main(api_key, north, south, east, west, max_results, max_comments):\n",
        "    location, radius = calculate_center_and_radius(north, south, east, west)\n",
        "    places = search_places(api_key, location, max_results)\n",
        "\n",
        "    all_reviews = []  # Initialize a list to store all reviews\n",
        "\n",
        "    if not places:\n",
        "        print(\"No places found within the specified area.\")\n",
        "        return\n",
        "\n",
        "    for place in places[:max_results]:  # Ensures you don't process more places than necessary\n",
        "        reviews = get_place_details(api_key, place['place_id'], max_comments)\n",
        "        if reviews == \"No comments found\":\n",
        "            print(\"No comments found for\", place['name'])\n",
        "        else:\n",
        "            print(f\"Comments for {place['name']}:\")\n",
        "            for review in reviews:\n",
        "                print(review)\n",
        "                all_reviews.append((place['name'], review))  # Save reviews into the list\n",
        "\n",
        "    return all_reviews  # Return the list of all reviews\n",
        "\n",
        "\n",
        "# Example use case\n",
        "api_key = 'AIzaSyBfcDwI-86zy3jZ00uwnsuxAwUjD8J0kCw'\n",
        "\n",
        "north = bbox[3]\n",
        "south = bbox[1]\n",
        "east = bbox[2]\n",
        "west = bbox[0]\n",
        "\n",
        "initial_radius = max(abs(north - south), abs(east - west)) / 2 * 111000\n",
        "\n",
        "all_reviews = main(api_key, north, south, east, west, max_results=120, max_comments=10)\n",
        "\n",
        "#Print the reviews\n",
        "for review in all_reviews:\n",
        "    print( review)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGLHELc6Ip2O"
      },
      "source": [
        "# ***NLTK Processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK1MKUxp4Qiw"
      },
      "source": [
        "*** / Generate symonyms and fine tune LLM***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIdMyX9T4Vvl"
      },
      "outputs": [],
      "source": [
        "!pip install nltk gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0GhgRjG5aPs"
      },
      "source": [
        "Tokenize Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D05_h12f5MG_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download necessary datasets\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "input_text = \"find a indoor place that has a lot of windows, or it can be outdoor too. It should be close to the the park and university, relaxing and conforting vibe, people can rest, and it should also be clean. \"\n",
        "tokens = tokenize_text(input_text)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vST_njl5dY1"
      },
      "source": [
        "***Build Synonyms***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYSqtdZhHXrj"
      },
      "source": [
        "***Use Word2Vec***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMhXz9FSS6_f"
      },
      "source": [
        "***if input outter data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5ckqBxCN_Ek"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample comments in a list of tuples\n",
        "comments = []\n",
        "\n",
        "def normalize_text(text):\n",
        "    replacements = {\n",
        "        \"’\": \"'\",\n",
        "        \"‘\": \"'\",\n",
        "        \"“\": '\"',\n",
        "        \"”\": '\"',\n",
        "        \"\\u2013\": \"-\",  # en-dash\n",
        "        \"\\u2014\": \"-\",  # em-dash\n",
        "    }\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "    return text\n",
        "\n",
        "# Convert each tuple to a single string, normalize, and tokenize\n",
        "tokenized_sentences = [word_tokenize(normalize_text(comment)) for _, comment in comments]\n",
        "\n",
        "\n",
        "# Print the tokenized sentences\n",
        "print(tokenized_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ7Je63UAu23"
      },
      "outputs": [],
      "source": [
        "# Convert each tuple to a string and tokenize\n",
        "tokenized_sentences = [word_tokenize(\"\".join(sentence).lower()) for sentence in all_reviews]\n",
        "\n",
        "print(tokenized_sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lBTXnhEHmMz"
      },
      "source": [
        "Train Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcA2vDk9_ncS"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')  # For tokenization\n",
        "nltk.download('stopwords')  # For stopwords\n",
        "\n",
        "#Clen data(punctuations and stopwords)\n",
        "def clean_tokens(sentences):\n",
        "    # Load English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Combine stopwords with punctuation\n",
        "    stop_words.update(string.punctuation)\n",
        "\n",
        "    # Filter out stopwords and punctuation from each tokenized sentence\n",
        "    cleaned_sentences = [[word for word in sentence if word not in stop_words] for sentence in sentences]\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "# Convert each review to a list of words, then clean it\n",
        "tokenized_sentences = [word_tokenize(\" \".join(sentence).lower()) for sentence in all_reviews]\n",
        "cleaned_sentences = clean_tokens(tokenized_sentences)\n",
        "\n",
        "print(cleaned_sentences)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Training the model\n",
        "model = Word2Vec(cleaned_sentences, vector_size=100, window=10, min_count=0.1, workers=10)\n",
        "\n",
        "# Save the model for later use\n",
        "model.save(\"word2vec_model.model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v69JFsvkHq8C"
      },
      "source": [
        "***Find closely-related words***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAxwjYl5Hz0Q"
      },
      "source": [
        "Find top synonyms of the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF5FgcNyUOic"
      },
      "outputs": [],
      "source": [
        "def find_top_related_words(tokens, model):\n",
        "    related_words = {}\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            # Retrieve the top three most similar words for each token\n",
        "            similar_words = model.wv.most_similar(token, topn=3)\n",
        "            related_words[token] = similar_words\n",
        "        except KeyError:\n",
        "            # Handle the case where the token is not in the model's vocabulary\n",
        "            continue\n",
        "    return related_words\n",
        "\n",
        "def remove_stopwords_and_punctuation(tokenized_sentence):\n",
        "    # Load stopwords for English (you can change the language as needed)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Include all punctuation in the set of characters to remove\n",
        "    stop_words.update(string.punctuation)\n",
        "\n",
        "    # Filter out any words that are in the list of stopwords or are punctuations\n",
        "    cleaned_sentence = [word for word in tokenized_sentence if word not in stop_words]\n",
        "\n",
        "    return cleaned_sentence\n",
        "\n",
        "# Clean the sentence\n",
        "cleaned_input = remove_stopwords_and_punctuation(tokens)\n",
        "print(cleaned_input)\n",
        "\n",
        "\n",
        "# Load the trained model\n",
        "model = Word2Vec.load(\"word2vec_model.model\")\n",
        "\n",
        "related_words = find_top_related_words(cleaned_input, model)\n",
        "print(related_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV1RyTIAh4L5"
      },
      "source": [
        "***Gather all useful words into a list***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_K4HjYS5Yw0"
      },
      "source": [
        "***Remove prompt if not in the comments***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tPrB4tykr8j"
      },
      "outputs": [],
      "source": [
        "selected_words = []\n",
        "\n",
        "flattened_valid_words = [word for sublist in cleaned_sentences for word in sublist]\n",
        "\n",
        "print(flattened_valid_words)\n",
        "for key, values in related_words.items():\n",
        "    # Check if the key is in the valid_words list\n",
        "    if key in flattened_valid_words:\n",
        "        selected_words.append(key)\n",
        "        print(key) # Append the key only if it's in valid_words\n",
        "    for value in values:\n",
        "        selected_words.append(value[0])  # Append the first element of each tuple\n",
        "\n",
        "print(selected_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q5F0iHlT9vk"
      },
      "source": [
        "***General wordnet result***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIFglZ7nKqHP"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "# Download necessary WordNet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def find_synonyms(words):\n",
        "    all_synonyms = {}\n",
        "    for word in words:\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name().replace('_', ' '))  # Replace underscores for multi-word synonyms\n",
        "        all_synonyms[word] = list(synonyms)\n",
        "    return all_synonyms\n",
        "\n",
        "# Example usage with multiple words\n",
        "words = cleaned_input\n",
        "synonyms = find_synonyms(words)\n",
        "for word, syn_list in synonyms.items():\n",
        "    print(f\"Synonyms for {word}: {syn_list}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRZCOszC7dgB"
      },
      "source": [
        "# ***Street View Segmentation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtV3JqvIATRx"
      },
      "source": [
        "***Download GroundingDino***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vAPuAJk_l2v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH_VdqB3-ljc"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!pip install -q -e .\n",
        "!pip install -q roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS0uhjv8-6bZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iefWJnzwAQww"
      },
      "source": [
        "***Download weights***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6Rd4es2AKCV"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "!mkdir {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmPX6-FgAWnd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjYN6AvZAd9w"
      },
      "source": [
        "***Load GroundingDino data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7RkPHA-AZzz"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_95JajnMAvNK"
      },
      "source": [
        "***Segmentation starts here***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBFlL66kbSnc"
      },
      "outputs": [],
      "source": [
        "selected_words = ['find', 'lines', 'mess', 'shocked', 'place','san', 'service', 'close', 'italy', 'official', 'nights', 'park', 'late', 'beautiful', 'really', 'relaxing', 'haircut', 'limited', 'vibe', 'judge',  'terre', 'people', 'rest', 'stars', 'highlighted', 'daily',  'terre', 'clean', 'classic', 'winding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "9t1glNxreOC1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HOME = os.getcwd()\n",
        "data_folder = \"/content/images\"\n",
        "output_folder = os.path.join(HOME, 'output_images')  # Directory to save output images\n",
        "os.makedirs(output_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# Constants\n",
        "TEXT_PROMPT = ' '.join(selected_words)  # Ensure 'selected_words' is defined\n",
        "BOX_THRESHOLD = 0.1\n",
        "TEXT_THRESHOLD = 0.1\n",
        "\n",
        "# Get a list of image files\n",
        "image_files = [f for f in os.listdir(data_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# List to hold the output data\n",
        "output_data = []\n",
        "\n",
        "# Process each image\n",
        "for image_name in image_files:\n",
        "    image_path = os.path.join(data_folder, image_name)\n",
        "    image_source, image = load_image(image_path)  # Define load_image function or adjust accordingly if not defined\n",
        "\n",
        "    boxes, logits, phrases = predict(\n",
        "        model=model,\n",
        "        image=image,\n",
        "        caption=TEXT_PROMPT,\n",
        "        box_threshold=BOX_THRESHOLD,\n",
        "        text_threshold=TEXT_THRESHOLD\n",
        "    )\n",
        "\n",
        "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)  # Define annotate function or adjust accordingly\n",
        "\n",
        "    # Store the image name and the confidence score of each bounding box\n",
        "    image_data = [image_name] + [logit.max().item() for logit in logits]\n",
        "    output_data.append(image_data)\n",
        "\n",
        "    # Display and save the annotated image\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(annotated_frame)  # Adjust this line if sv.plot_image does not work directly\n",
        "    plt.show()\n",
        "    save_path = os.path.join(output_folder, f'annotated_{image_name}')  # Path to save the image\n",
        "    plt.savefig(save_path)  # Save the figure to file\n",
        "    plt.close()  # Close the figure to free memory\n",
        "\n",
        "    # Save the dataset to a text file outside of the output_images folder\n",
        "    dataset_path = os.path.join(HOME, 'dataset.txt')  # Adjust this path if you want it saved elsewhere\n",
        "    with open(dataset_path, 'w') as file:\n",
        "      for data in output_data:\n",
        "        file.write(f\"{data}\\n\")  # Writing each entry in a new line\n",
        "\n",
        "# Print the dataset\n",
        "for data in output_data:\n",
        "    print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ayHoo3BqKVKQ"
      },
      "outputs": [],
      "source": [
        "!zip -r output.zip images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD15qpZeib7r"
      },
      "source": [
        "#***Aerial View Segment***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD-CTa_lwkTw"
      },
      "source": [
        "***Convert tiff to jpg***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm3t6_uiwF7p"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Define the full path to the .tif file\n",
        "tif_file_path = \"/content/Image.tif\"\n",
        "\n",
        "# Define the full path where the .jpg file should be saved\n",
        "jpg_file_path = \"/content/NewImage.jpg\"\n",
        "\n",
        "# Load the TIFF image\n",
        "image = Image.open(tif_file_path)\n",
        "\n",
        "# Convert the image to JPEG and save it\n",
        "image.convert('RGB').save(jpg_file_path, 'JPEG', quality=90)  # Adjust the quality as needed\n",
        "\n",
        "print(f\"Converted image saved at: {jpg_file_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPmFzwDwrb7"
      },
      "source": [
        "***Segmentation starts***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObLc7sSiVAQ0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Constants\n",
        "TEXT_PROMPT = ' '.join(selected_words)\n",
        "BOX_THRESHOLD = 0.1\n",
        "TEXT_THRESHOLD = 0.1\n",
        "\n",
        "# Directly specify the path to the image\n",
        "image_path = \"/content/NewImage.jpg\"  # Update this to the exact path of your image\n",
        "\n",
        "# Load the image\n",
        "image = Image.open(image_path)\n",
        "image_source = image.copy()  # Preserve the original image for annotation\n",
        "\n",
        "# Dummy prediction function\n",
        "def predict(model, image, caption, box_threshold, text_threshold):\n",
        "    # Example output for demonstration\n",
        "    boxes = [(20, 20, 70, 70)]  # Example box coordinates (x, y, x+w, y+h)\n",
        "    logits = [4.5]  # Confidence scores\n",
        "    phrases = ['example phrase']  # Detected phrases\n",
        "    return boxes, logits, phrases\n",
        "\n",
        "# Dummy annotation function modified to not show text\n",
        "def annotate(image_source, boxes, logits, phrases):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image_source)\n",
        "    for box in boxes:\n",
        "        plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor='red', linewidth=2))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    return image_source\n",
        "\n",
        "# Run prediction\n",
        "boxes, logits, phrases = predict(\n",
        "    model=None,  # Define your model or replace with an actual model call\n",
        "    image=image,\n",
        "    caption=TEXT_PROMPT,\n",
        "    box_threshold=BOX_THRESHOLD,\n",
        "    text_threshold=TEXT_THRESHOLD\n",
        ")\n",
        "\n",
        "# Annotate the image\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "# Prepare dataset output\n",
        "image_data = [os.path.basename(image_path)] + [logit for logit in logits]\n",
        "dataset = [image_data]\n",
        "\n",
        "# Output the dataset\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ2sqvYDZBWZ"
      },
      "outputs": [],
      "source": [
        "from samgeo.text_sam import LangSAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_khX2Zt9bPl5"
      },
      "outputs": [],
      "source": [
        "sam = LangSAM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnXbLxowc7ok"
      },
      "outputs": [],
      "source": [
        "text_prompt = ' '.join(selected_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d326CT5zcJtu"
      },
      "outputs": [],
      "source": [
        "sam.predict(image, text_prompt, box_threshold=0.1, text_threshold=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEr952pElgRv"
      },
      "outputs": [],
      "source": [
        "# Prepare an empty list to hold your dataset entries\n",
        "dataset = []\n",
        "\n",
        "# Extract data from each detected object\n",
        "for box, logit in zip(boxes, logits):\n",
        "    # Extract bounding box coordinates\n",
        "    x1, y1, x2, y2 = box\n",
        "    # Extract confidence score\n",
        "    confidence_score = logit\n",
        "\n",
        "    # Create a dictionary for each detected object\n",
        "    detection_data = {\n",
        "        \"Coordinates\": (x1, y1, x2, y2),\n",
        "        \"Confidence Score\": confidence_score\n",
        "    }\n",
        "\n",
        "    # Append the dictionary to the dataset\n",
        "    dataset.append(detection_data)\n",
        "\n",
        "# Print or output the dataset\n",
        "for data in dataset:\n",
        "    print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLfuYVnGdHC1"
      },
      "outputs": [],
      "source": [
        "sam.show_anns(\n",
        "    cmap=\"Greens\",\n",
        "    box_color=\"red\",\n",
        "    title=\"Automatic Segmentation\",\n",
        "    blend=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTEufQc9iYn2"
      },
      "source": [
        "# ***Mapping***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch8fc91PibDR"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib numpy basemap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o17qLEzPqCTq"
      },
      "source": [
        "***Get image coords and score***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4WM13DXtz5O"
      },
      "outputs": [],
      "source": [
        "# Path to the text file\n",
        "file_path = '/content/segSV.txt'  # Update this to the path of your text file\n",
        "\n",
        "# Read the data from the text file\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Clean up the line to remove any leading/trailing whitespace and newline characters\n",
        "        # Then split the line by commas\n",
        "        line_cleaned = line.strip().replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
        "        data.append(line_cleaned.split(','))\n",
        "\n",
        "# Process each entry to extract coordinates and calculate average confidence\n",
        "formatted_data = []\n",
        "\n",
        "for entry in data:\n",
        "    # Extract filename and remove the '.jpg' suffix to get the raw coordinates string\n",
        "    filename = entry[0].strip()\n",
        "    coords = filename[:-4]  # Remove '.jpg'\n",
        "    latitude, longitude = coords.split('_')\n",
        "    latitude = round(float(latitude), 4)  # Round latitude to four decimal places\n",
        "    longitude = round(float(longitude), 4)  # Round longitude to four decimal places\n",
        "\n",
        "    # Convert each string in the list to a float before calculating the average\n",
        "    # Start from entry[1] to skip the filename\n",
        "    confidence_scores = [float(score.strip()) for score in entry[1:]]\n",
        "    average_confidence = sum(confidence_scores) / len(confidence_scores)\n",
        "\n",
        "    # Round the average confidence to three decimal places\n",
        "    average_confidence = round(average_confidence, 3)\n",
        "\n",
        "    # Create a tuple and add it to the list\n",
        "    formatted_data.append((latitude, longitude, average_confidence))\n",
        "\n",
        "# Print the formatted data\n",
        "for item in formatted_data:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj_q-7DAtt1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "\n",
        "coordinates = formatted_data  # This should be a list of tuples (lat, lon, conf)\n",
        "\n",
        "# Create a new map plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Define the bounding box for the map\n",
        "lat_min, lat_max = bbox[3], bbox[1]\n",
        "lon_min, lon_max = bbox[2], bbox[0]\n",
        "\n",
        "# Create the map\n",
        "m = Basemap(projection='merc', llcrnrlat=lat_min, urcrnrlat=lat_max, llcrnrlon=lon_min, urcrnrlon=lon_max, resolution='i', ax=ax)\n",
        "m.drawmapboundary(fill_color='aqua')\n",
        "m.fillcontinents(color='coral', lake_color='aqua')\n",
        "\n",
        "# Correct the use of map projection and handle the coordinates and confidence scores properly\n",
        "x, y = [], []\n",
        "weights = []\n",
        "for lat, lon, conf in coordinates:\n",
        "    x_proj, y_proj = m(lon, lat)  # Get projected x, y from longitude and latitude\n",
        "    x.append(x_proj)\n",
        "    y.append(y_proj)\n",
        "    weights.append(conf)  # Add confidence to the weights list\n",
        "\n",
        "# Create the heatmap\n",
        "heatmap, xedges, yedges = np.histogram2d(x, y, bins=50, weights=weights, density=True)\n",
        "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
        "\n",
        "plt.imshow(heatmap.T, extent=extent, origin='lower', cmap='plasma', alpha=0.6)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYjK1a4ZpeEH"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import numpy as np\n",
        "\n",
        "# Example coordinates\n",
        "coordinates = formatted_data  # Your list of tuples [(lat, lon, conf), ...]\n",
        "\n",
        "# Calculate bounds for the map view\n",
        "latitudes, longitudes = zip(*[(lat, lon) for lat, lon, conf in coordinates])\n",
        "southwest = [min(latitudes), min(longitudes)]\n",
        "northeast = [max(latitudes), max(longitudes)]\n",
        "\n",
        "# Create a map centered around the average location\n",
        "map_center = np.mean(latitudes), np.mean(longitudes)\n",
        "map = folium.Map(location=map_center, tiles='CartoDB positron', attr='Minimal')\n",
        "\n",
        "# Add a heatmap\n",
        "heatmap_data = [(lat, lon, conf) for lat, lon, conf in coordinates]\n",
        "HeatMap(heatmap_data).add_to(map)\n",
        "\n",
        "# Fit map to bounds\n",
        "map.fit_bounds([southwest, northeast])\n",
        "\n",
        "# Save to HTML or display\n",
        "map.save('heatmap.html')\n",
        "map\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
